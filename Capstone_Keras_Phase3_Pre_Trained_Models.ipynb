{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item31'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Import Libraries and Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item32'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "For your convenience, I have placed the data on a server which you can retrieve easily using the **wget** command. So let's run the following line of code to get the data. Given the large size of the image dataset, it might take some time depending on your internet speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-03 19:07:42--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 261482368 (249M) [application/zip]\n",
      "Saving to: ‘concrete_data_week3.zip’\n",
      "\n",
      "concrete_data_week3 100%[===================>] 249.37M  23.1MB/s    in 11s     \n",
      "\n",
      "2020-09-03 19:07:53 (23.6 MB/s) - ‘concrete_data_week3.zip’ saved [261482368/261482368]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now if you check the left directory pane, you should see the zipped file *concrete_data_week3.zip* appear. So, let's go ahead and unzip the file to access the images. Given the large number of images in the dataset, this might take a couple of minutes, so please be patient, and wait until the code finishes running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  concrete_data_week3.zip\n",
      "replace concrete_data_week3/valid/positive/16679_1.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!unzip concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50*** error. So please **DO NOT DO IT**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item33'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Define Global Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item34'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Construct ImageDataGenerator Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19998 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item35'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Build, Compile and Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7f4a70b91860>,\n",
       " <keras.layers.core.Dense at 0x7f4b03e9dd68>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the ResNet50 layers by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f4b03e9def0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f4b03eb3a58>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4b03eb3a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4b03eb3b70>,\n",
       " <keras.layers.core.Activation at 0x7f4b2c6abf60>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f4bc6247c88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4b03ebf358>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4b2c680860>,\n",
       " <keras.layers.core.Activation at 0x7f4b2c6d6518>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4bc47eadd8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4bc47d8ef0>,\n",
       " <keras.layers.core.Activation at 0x7f4bc472a208>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4bc46f5668>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4bc43d7a58>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4bc46e3780>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4bc43a6fd0>,\n",
       " <keras.layers.merge.Add at 0x7f4bc4314fd0>,\n",
       " <keras.layers.core.Activation at 0x7f4b2bfe5d68>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4b2bfe5ba8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4b2bf20fd0>,\n",
       " <keras.layers.core.Activation at 0x7f4b00633470>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a73dd1fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a73dac780>,\n",
       " <keras.layers.core.Activation at 0x7f4a73d083c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a73ce1860>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a73cd0978>,\n",
       " <keras.layers.merge.Add at 0x7f4a73c03c18>,\n",
       " <keras.layers.core.Activation at 0x7f4a73bd5eb8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a73bd5fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a73bbe7b8>,\n",
       " <keras.layers.core.Activation at 0x7f4a73b1b8d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a73afcba8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a73aeecc0>,\n",
       " <keras.layers.core.Activation at 0x7f4a73a18dd8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a73992400>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a7394ec88>,\n",
       " <keras.layers.merge.Add at 0x7f4a7392e828>,\n",
       " <keras.layers.core.Activation at 0x7f4a73883da0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a73883be0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a73869e10>,\n",
       " <keras.layers.core.Activation at 0x7f4a737d64a8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a737a4780>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a73791898>,\n",
       " <keras.layers.core.Activation at 0x7f4a736c0b38>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a736a7fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a735e9400>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a7367b748>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a73582978>,\n",
       " <keras.layers.merge.Add at 0x7f4a7350ea90>,\n",
       " <keras.layers.core.Activation at 0x7f4a734a9710>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a734a9550>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a73401cf8>,\n",
       " <keras.layers.core.Activation at 0x7f4a733e0dd8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a73350978>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a733bfa90>,\n",
       " <keras.layers.core.Activation at 0x7f4a732f0d30>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a73254d30>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a7322ea90>,\n",
       " <keras.layers.merge.Add at 0x7f4a731875c0>,\n",
       " <keras.layers.core.Activation at 0x7f4a73154b70>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a731549b0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a7317b198>,\n",
       " <keras.layers.core.Activation at 0x7f4a730ab080>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a73079550>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a7303beb8>,\n",
       " <keras.layers.core.Activation at 0x7f4a72f96940>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a72f7eda0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a72f6ceb8>,\n",
       " <keras.layers.merge.Add at 0x7f4a72e411d0>,\n",
       " <keras.layers.core.Activation at 0x7f4a72e7c748>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a72e7c588>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a72dc9d30>,\n",
       " <keras.layers.core.Activation at 0x7f4a72dace10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a72d11d68>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a72ced9b0>,\n",
       " <keras.layers.core.Activation at 0x7f4a72c484e0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a72c27978>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a72c16a90>,\n",
       " <keras.layers.merge.Add at 0x7f4a72b46d68>,\n",
       " <keras.layers.core.Activation at 0x7f4a72b16f28>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a72b16f98>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a72a838d0>,\n",
       " <keras.layers.core.Activation at 0x7f4a72a5e9e8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a72a3fc18>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a72a31b38>,\n",
       " <keras.layers.core.Activation at 0x7f4a729040f0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a728d3550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a72871908>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a7293e668>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a727c1eb8>,\n",
       " <keras.layers.merge.Add at 0x7f4a727aee10>,\n",
       " <keras.layers.core.Activation at 0x7f4a7273ecc0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a7273ee80>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a726baf28>,\n",
       " <keras.layers.core.Activation at 0x7f4a72615b38>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a725ff6d8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a725ef7f0>,\n",
       " <keras.layers.core.Activation at 0x7f4a7251da90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a72485f28>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a724f1cc0>,\n",
       " <keras.layers.merge.Add at 0x7f4a723c7320>,\n",
       " <keras.layers.core.Activation at 0x7f4a723848d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a72384710>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a7233af28>,\n",
       " <keras.layers.core.Activation at 0x7f4a722e23c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a722af6a0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a722997b8>,\n",
       " <keras.layers.core.Activation at 0x7f4a721cda90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a721b0ef0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a7219ec88>,\n",
       " <keras.layers.merge.Add at 0x7f4a720f42e8>,\n",
       " <keras.layers.core.Activation at 0x7f4a720b2898>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a720b26d8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a72000ef0>,\n",
       " <keras.layers.core.Activation at 0x7f4a71f910b8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a71f5a668>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a71f47780>,\n",
       " <keras.layers.core.Activation at 0x7f4a71efaa58>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a71e62eb8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a71e3be48>,\n",
       " <keras.layers.merge.Add at 0x7f4a71da34a8>,\n",
       " <keras.layers.core.Activation at 0x7f4a71d60860>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a71d606a0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a71d2ce80>,\n",
       " <keras.layers.core.Activation at 0x7f4a71cbc080>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a71c0a630>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a71c76748>,\n",
       " <keras.layers.core.Activation at 0x7f4a71ba69e8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a71b0de80>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a71b7af98>,\n",
       " <keras.layers.merge.Add at 0x7f4a71a502b0>,\n",
       " <keras.layers.core.Activation at 0x7f4a71a0e828>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a71a0e668>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a719ddeb8>,\n",
       " <keras.layers.core.Activation at 0x7f4a7196d048>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a7193a5f8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a71926710>,\n",
       " <keras.layers.core.Activation at 0x7f4a718549e8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a7183be48>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a71828f60>,\n",
       " <keras.layers.merge.Add at 0x7f4a7177c240>,\n",
       " <keras.layers.core.Activation at 0x7f4a717387f0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a71738630>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a71687dd8>,\n",
       " <keras.layers.core.Activation at 0x7f4a716190f0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a715e65c0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a715ce6d8>,\n",
       " <keras.layers.core.Activation at 0x7f4a715039b0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a714eae10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a7142d208>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a714d4f28>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a713e67b8>,\n",
       " <keras.layers.merge.Add at 0x7f4a71355908>,\n",
       " <keras.layers.core.Activation at 0x7f4a712eb940>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a712eb780>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a71228f98>,\n",
       " <keras.layers.core.Activation at 0x7f4a711d4438>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a71198f98>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a7118cd30>,\n",
       " <keras.layers.core.Activation at 0x7f4a710da3c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a710a7828>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a71097940>,\n",
       " <keras.layers.merge.Add at 0x7f4a70fcac18>,\n",
       " <keras.layers.core.Activation at 0x7f4a70f9cf60>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a70f9cfd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a70f07780>,\n",
       " <keras.layers.core.Activation at 0x7f4a70ee1c88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a70e44f60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a70ebccf8>,\n",
       " <keras.layers.core.Activation at 0x7f4a70d8b358>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f4a70d5a7f0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f4a70d4b908>,\n",
       " <keras.layers.merge.Add at 0x7f4a70cfbbe0>,\n",
       " <keras.layers.core.Activation at 0x7f4a70c49dd8>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7f4a70c49fd0>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7f4a70c356d8>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next we compile our model using the **adam** optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "  1/200 [..............................] - ETA: 12:19:01 - loss: 0.2389 - acc: 0.9100"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
